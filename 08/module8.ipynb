{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Le_net class to train and save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "class LeNet:\n",
    "    def __init__(self, batch_size=32, epochs=20):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        self._create_lenet()\n",
    "        self._compile()\n",
    "\n",
    "    def _create_lenet(self):\n",
    "    \n",
    "        self.model = Sequential([\n",
    "            Conv2D(filters=6, kernel_size=(5, 5), activation='sigmoid',\n",
    "                   input_shape=(28, 28, 1), padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "            \n",
    "            Conv2D(filters=16, kernel_size=(5, 5), activation='sigmoid', padding='same'),\n",
    "            AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "            \n",
    "            Flatten(),\n",
    "            Dense(120, activation='sigmoid'),\n",
    "            Dense(84, activation='sigmoid'),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def _compile(self):\n",
    "        \"\"\"\n",
    "        Compile the LeNet model with an Adam optimizer and categorical crossentropy loss.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError('Error: Model is not created yet.')\n",
    "        \n",
    "        self.model.compile(optimizer='Adam',\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"\n",
    "        Preprocess the MNIST dataset: normalize, reshape, and one-hot encode the labels.\n",
    "        \"\"\"\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        \n",
    "        # Normalize the pixel values to the range [0, 1]\n",
    "        x_train = x_train / 255.0\n",
    "        x_test = x_test / 255.0\n",
    "        \n",
    "        # Add channel dimension\n",
    "        self.x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "        self.x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "        \n",
    "        # One-hot encode the labels\n",
    "        self.y_train = to_categorical(y_train, 10)\n",
    "        self.y_test = to_categorical(y_test, 10)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model using the preprocessed MNIST dataset.\n",
    "        \"\"\"\n",
    "        self._preprocess()\n",
    "        self.model.fit(self.x_train, self.y_train, \n",
    "                       batch_size=self.batch_size, \n",
    "                       epochs=self.epochs,\n",
    "                       validation_data=(self.x_test, self.y_test))\n",
    "\n",
    "    def save(self, model_path_name):\n",
    "        \"\"\"\n",
    "        Save the trained model to the specified file path.\n",
    "        \"\"\"\n",
    "        if not model_path_name.endswith(\".keras\"):\n",
    "            model_path_name += \".keras\"\n",
    "        self.model.save(model_path_name)\n",
    "        print(f\"Model saved as {model_path_name}\")\n",
    "\n",
    "    def load(self, model_path_name):\n",
    "        \"\"\"\n",
    "        Load a saved model from the specified file path.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(model_path_name):\n",
    "            raise FileNotFoundError(f\"No such file: {model_path_name}\")\n",
    "        self.model = load_model(model_path_name)\n",
    "        print(f\"Model loaded from {model_path_name}\")\n",
    "\n",
    "    def predict(self, images):\n",
    "        \"\"\"\n",
    "        Predict the class probabilities for a list of images.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model is not loaded or created yet.\")\n",
    "        \n",
    "        # Preprocess images: normalize and add channel dimension\n",
    "        images = [img / 255.0 for img in images]\n",
    "        images = [img.reshape(1, 28, 28, 1) for img in images]\n",
    "        \n",
    "        # Run predictions and return results\n",
    "        predictions = [self.model.predict(img, verbose=0) for img in images]\n",
    "        return [pred.argmax() for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\anaconda3\\envs\\ece5831-2024\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lenet = LeNet(batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.3668 - loss: 1.7505 - val_accuracy: 0.9073 - val_loss: 0.3145\n",
      "Epoch 2/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9161 - loss: 0.2823 - val_accuracy: 0.9428 - val_loss: 0.1941\n",
      "Epoch 3/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9441 - loss: 0.1806 - val_accuracy: 0.9616 - val_loss: 0.1290\n",
      "Epoch 4/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9592 - loss: 0.1341 - val_accuracy: 0.9665 - val_loss: 0.1029\n",
      "Epoch 5/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9680 - loss: 0.1054 - val_accuracy: 0.9705 - val_loss: 0.0964\n",
      "Epoch 6/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9736 - loss: 0.0863 - val_accuracy: 0.9789 - val_loss: 0.0693\n",
      "Epoch 7/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9772 - loss: 0.0757 - val_accuracy: 0.9785 - val_loss: 0.0672\n",
      "Epoch 8/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9794 - loss: 0.0638 - val_accuracy: 0.9837 - val_loss: 0.0553\n",
      "Epoch 9/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9821 - loss: 0.0582 - val_accuracy: 0.9823 - val_loss: 0.0540\n",
      "Epoch 10/10\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9844 - loss: 0.0532 - val_accuracy: 0.9833 - val_loss: 0.0546\n"
     ]
    }
   ],
   "source": [
    "lenet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as RDewalkar.keras\n"
     ]
    }
   ],
   "source": [
    "lenet.save(\"RDewalkar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from RDewalkar.keras\n"
     ]
    }
   ],
   "source": [
    "lenet.load(\"RDewalkar.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = lenet.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(7),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(4),\n",
       " np.int64(1),\n",
       " np.int64(4),\n",
       " np.int64(9),\n",
       " np.int64(5),\n",
       " np.int64(9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Hand Written Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def process_images_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load and process all images from a folder, ensuring each image is (28, 28) (grayscale).\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed images, each with shape (28, 28).\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure it's a valid image file\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image = cv2.imread(file_path)  # Load image\n",
    "            \n",
    "            if image is not None:  # Check if image is loaded successfully\n",
    "                # Convert to grayscale (if it's not already)\n",
    "                grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Resize to 28x28\n",
    "                resized_image = cv2.resize(grayscale_image, (28, 28))\n",
    "                \n",
    "                # Append to the list\n",
    "                processed_images.append(resized_image)\n",
    "    \n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D://Masters//Pattern Recognition and Neural Networks//0_images\" \n",
    "test_images = process_images_from_folder(folder_path)\n",
    "\n",
    "# Predict with LeNet\n",
    "predictions = lenet.predict(test_images)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
